{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "nlp = spacy.load('ro_core_news_lg', disable=[\"tagger\", \"attribute_ruler\", \"tok2vec\", \"ner\"])\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace and for romanian language lemmatisation\"\"\"\n",
    "\n",
    "    def lemma(text: str) -> str:\n",
    "        my_doc = nlp(text)\n",
    "\n",
    "        return ' '.join([token.lemma_ for token in my_doc])\n",
    "\n",
    "    def remove_articles(text: str) -> str:\n",
    "        return re.sub(r'\\b(a|an|the|un|o)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text: str) -> str:\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text: str) -> str:\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    return lemma(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
    "\n",
    "\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction: str, ground_truth: str) -> bool:\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn: Callable, prediction: str, ground_truths: List[str]) -> float:\n",
    "    scores_for_ground_truths = []\n",
    "\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(dir_generate: str) -> None:\n",
    "    type_model = dir_generate.split('/')[-1]\n",
    "    results = {}\n",
    "\n",
    "    for i in [str(x) for x in Path(dir_generate).glob(\"*\") if x.is_dir()]:\n",
    "        for path_file in [str(x) for x in Path(i).glob('**/*.json') if x.is_file()]:\n",
    "            f1, exact_match, total = 0, 0, 0\n",
    "            name_eval = path_file.split('/')[-1].replace('.json', '')\n",
    "\n",
    "            with open(path_file, 'r') as input:\n",
    "                data = json.load(input)\n",
    "\n",
    "            for example in data:\n",
    "                ground_truths = [example['original']]\n",
    "                prediction = example['predict']\n",
    "\n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                total += 1\n",
    "\n",
    "            exact_match = 100.0 * exact_match / total\n",
    "            f1 = 100.0 * f1 / total\n",
    "\n",
    "            results[name_eval] = {'exact_match': exact_match, 'f1': f1}\n",
    "            print(f'Eval: {name_eval}, Exact_Match: {exact_match}, F1: {f1}')\n",
    "\n",
    "    Path('../../../log/xquad/').mkdir(exist_ok=True, parents=True)\n",
    "    with open(f'../../../log/xquad/{type_model}.txt', 'w+') as output_file:\n",
    "        for k, v in results.items():\n",
    "            output_file.write(f'{k}: {v}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: base-beam-search-4, Exact_Match: 24.11764705882353, F1: 35.27481850314759\n",
      "Eval: base-greedy, Exact_Match: 23.69747899159664, F1: 35.970264064383905\n",
      "Eval: base-beam-search-8, Exact_Match: 24.11764705882353, F1: 35.27481850314759\n",
      "Eval: medium-beam-search-4, Exact_Match: 31.596638655462186, F1: 45.32667118672392\n",
      "Eval: medium-beam-search-8, Exact_Match: 31.596638655462186, F1: 45.32667118672392\n",
      "Eval: medium-greedy, Exact_Match: 29.66386554621849, F1: 44.740962768922714\n",
      "Eval: large-beam-search-4, Exact_Match: 29.66386554621849, F1: 43.05865381544039\n",
      "Eval: large-beam-search-8, Exact_Match: 29.66386554621849, F1: 43.05865381544039\n",
      "Eval: large-greedy, Exact_Match: 29.747899159663866, F1: 42.98861526665083\n"
     ]
    }
   ],
   "source": [
    "evaluate('../../../generate/xquad/normal')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: large-v2-top-7, Exact_Match: 31.34453781512605, F1: 44.71327852976019\n",
      "Eval: large-v2-greedy, Exact_Match: 31.34453781512605, F1: 44.71327852976019\n",
      "Eval: large-v2-beam-search-4, Exact_Match: 31.596638655462186, F1: 43.532352329176156\n",
      "Eval: large-v1-beam-search-8, Exact_Match: 28.73949579831933, F1: 39.71153009877786\n",
      "Eval: large-v1-top-7, Exact_Match: 28.403361344537814, F1: 39.790719779626826\n",
      "Eval: base-v1-greedy, Exact_Match: 23.865546218487395, F1: 34.27249347982264\n",
      "Eval: base-v1-top-7, Exact_Match: 23.865546218487395, F1: 34.27249347982264\n",
      "Eval: base-v1-beam-search-4, Exact_Match: 25.04201680672269, F1: 34.51705084886004\n",
      "Eval: base-v1-beam-search-8, Exact_Match: 25.04201680672269, F1: 34.51705084886004\n",
      "Eval: medium-v1-beam-search-8, Exact_Match: 27.647058823529413, F1: 39.11011390158698\n",
      "Eval: medium-v1-greedy, Exact_Match: 27.058823529411764, F1: 39.75278341376798\n",
      "Eval: medium-v1-top-7, Exact_Match: 27.058823529411764, F1: 39.75278341376798\n",
      "Eval: medium-v1-beam-search-4, Exact_Match: 27.647058823529413, F1: 39.11011390158698\n"
     ]
    }
   ],
   "source": [
    "evaluate('../../../generate/xquad/translate')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}